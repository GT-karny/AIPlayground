# 現在の実装ドキュメント

このドキュメントは、2026-02-14時点の `AIPlayground` の実装をコードベースから整理したものです。

## 1. 全体構成

- エントリーポイント: `main.py`
- UI: `app/chat_window.py`（PySide6）
- 非同期実行: `app/chat_worker.py`（QThread）
- 会話制御の中核: `app/api_client.py`（`ChatClient`）
- 意図判定: `app/intent_router.py`（`IntentRouter`）
- 検索ツール: `app/web_search.py`（`web_search` / tool schema / dispatch）
- 設定: `app/config.py`（`.env` 読み込み）
- テスト:
  - `tests/test_topic_selector.py`
  - `tests/test_eval_cases_from_md.py`
  - `tests/eval_cases.md`

## 2. 起動から応答までの処理

1. `main.py` で `QApplication` と `ChatWindow` を生成。
2. ユーザー送信時、`ChatWindow.send_message()` が `ChatWorker` を起動。
3. `ChatWorker.run()` が `ChatClient.send_message_with_tools()` を実行。
4. `status_callback` で UI ボタン文言とチャットログに進捗表示。
5. 応答完了で `response_ready` を返し、UIへ表示。

## 3. ChatClient のロジック

### 3.1 内部状態

- `self.messages`: OpenAI形式の会話履歴。初期値は人格を定義した system message。
- `turn_id`: `turn-1`, `turn-2` の形式で付与し、同一ターン内の user/assistant/tool を関連付け。

### 3.2 応答モード決定（Intent Router）

`IntentRouter.classify()` が履歴+最新入力を JSON で分類します。許可モード:

- `chat`: 雑談寄り
- `factual_balanced`: 事実寄り（ただし厳格すぎない）
- `factual_strict`: 出典重視の厳格モード
- `needs_clarification`: 確認質問を返す

同時に以下も返します。

- `use_rewrite` / `rewritten_user_message`: 文脈補完した質問文
- `reset_context`: 文脈リセット指示
- `confidence`, `reason` など

### 3.3 トピック選択（Topic Selector）

`ChatClient._select_topic_context()` が直近ターン候補から「今回参照すべき話題」を別途LLMで判定します。

- 入力: 元質問 + 補完後質問 + 直近ターン一覧
- 出力: `selected_turn_ids`, `topic_summary`, `confidence`
- 閾値: `TOPIC_SELECTOR_CONFIDENCE_THRESHOLD` 未満は破棄
- 無効条件: 新話題判定、ターン未選択、低信頼

ここで得た `topic_summary` は再検索クエリ計画や最終回答作成でヒントとして使われます。

### 3.4 factual系の一次経路（先行リサーチ）

`mode` が factual の場合、まず tool call ループに入る前に以下を実行します。

1. `_plan_research_queries()` で 1〜3 個の検索クエリを設計
2. `_run_research_queries()` で `web_search` を複数実行
3. `_append_research_payloads()` で tool結果を履歴に保存
4. `_summarize_research_results()` で回答文を生成
5. `_apply_self_check()` で自己検証・必要なら修復
6. strict で URL 未記載なら `_build_cited_fallback_from_tools()` へフォールバック

検索結果が取得できた場合はこの経路で回答を返し、tool call の反復ループには入りません。

### 3.5 tool call 反復ループ

先行リサーチで返せなかった場合、最大 `MAX_TOOL_ITERATIONS=3` で LLM の function calling を実行。

- `tools=TOOLS` を付与して応答生成
- tool call が来たら `execute_tool_call()` で実行し、toolメッセージとして履歴追加
- tool call がなければ回答確定（自己検証あり）

tool 周辺エラー（parser/validation系）は `_is_tooling_error()` で検知し、`_generate_without_tools()` に退避。

### 3.6 自己検証と修復

`_apply_self_check()` は以下の順で品質を保証します。

1. `_validate_answer()` で JSON 判定（ok/issues/repair_instruction）
2. ヒューリスティックで崩壊検知:
   - 空文字
   - 1800文字超
   - 長い反復ブロック
   - 同一文の多重反復
3. NGなら `_repair_output()` で再構成
4. `SELF_CHECK_MAX_RETRY` 回まで再検証

失敗時は issues を返し、factual系では `_attempt_autonomous_research_recovery()` により再検索復旧を試みます。

### 3.7 自律再検索復旧

検証で `factual_error/context_miss/non_answer/repetition` が出ると再検索を実施。

- クエリ再設計
- 追加検索
- 結果要約し再回答

この再検索結果も tool メッセージとして履歴に保持します。

## 4. web_search ツールの実装

`app/web_search.py` は DuckDuckGo Search (`ddgs`) を使った検索ツールです。

### 4.1 入出力

- 関数: `web_search(query, max_results=8)`
- 戻り値: JSON文字列
  - `query`
  - `query_variants`
  - `results[]`（title/url/domain/trust_level/snippet）

### 4.2 検索品質ロジック

- クエリ展開: `元クエリ`, `元クエリ 公式`, `元クエリ 一次情報`（最大3）
- レート制限: 0.5秒未満の連続呼び出しを拒否
- URL単位で重複除去
- ドメイン信頼度を算出:
  - `very_high/high/medium/low`
- スコア順で並べ替え:
  - 信頼度スコア + 複数バリアントでのヒット回数ボーナス
- low trust を後段化（十分な strong があれば low を落とす）

## 5. UI実装

`ChatWindow` はシンプルな1画面構成です。

- `QTextEdit`: 会話ログ表示（読み取り専用）
- `QLineEdit`: 入力
- `QPushButton`: 送信/進捗表示
- 送信中は入力を無効化
- status 変化時に `[検索中...]` などをログ表示

## 6. 設定値（主要）

`app/config.py` は `.env` を読み込み、以下を制御します。

- LLM接続: `BASE_URL`, `API_KEY`, `MODEL_NAME`
- 生成長/温度: `CHAT_MAX_TOKENS`, `FACTUAL_MAX_TOKENS`, `TEMPERATURE`
- ルーター: `ROUTER_MODEL_NAME`, `ROUTER_MAX_TOKENS`, `ROUTER_TEMPERATURE`
- 自己検証: `ENABLE_SELF_CHECK`, `SELF_CHECK_MAX_RETRY`, `SELF_CHECK_MAX_TOKENS`
- 自動再検索: `AUTO_RESEARCH_ENABLED`, `AUTO_RESEARCH_MAX_QUERIES`, `AUTO_RESEARCH_MAX_RESULTS`
- 話題選択: `TOPIC_SELECTOR_ENABLED`, `TOPIC_SELECTOR_MAX_TURNS`, `TOPIC_SELECTOR_MAX_TOKENS`, `TOPIC_SELECTOR_CONFIDENCE_THRESHOLD`

## 7. テスト実装

### 7.1 `tests/test_topic_selector.py`

- Topic selector の単体テスト（OpenAI 呼び出しをモック）
- 検証点:
  - LLMが返した `selected_turn_ids/topic_summary/confidence` の反映
  - 低信頼時の破棄
  - 新話題時の空結果

### 7.2 `tests/test_eval_cases_from_md.py`

- `tests/eval_cases.md` からケースを読み込み実行
- 実際に `ChatClient.send_message_with_tools()` を回し、最終回答を評価
- 既定では AI judge で pass/fail 判定（環境変数で無効化可）

`eval_cases.md` には以下観点が含まれます。

- 文脈解決
- 曖昧質問の確認
- 反復崩壊ガード
- strict factual
- casual chat
- self-check 各種
- topic shift / reset context

## 8. 補助スクリプト

`scripts/liquid_tool_parser.py` は vLLM の tool parser 拡張です。

- `<|tool_call_start|>` / `<|tool_call_end|>` で囲まれた出力を正規化してから既存 Pythonic parser へ渡す
- 通常抽出・ストリーミング抽出の両方を対応

## 9. 現状の実装特性（要点）

- factual系は「先行リサーチ経路」が優先され、結果が取れればそのまま返す。
- 失敗時も「自己検証→修復→再検索復旧→引用フォールバック」の多段フェイルセーフがある。
- 会話履歴は `turn_id` 付きで統合管理され、topic selector と再検索の参照に使われる。
- UIは軽量だが、進捗表示と非同期応答で実用性を確保している。
